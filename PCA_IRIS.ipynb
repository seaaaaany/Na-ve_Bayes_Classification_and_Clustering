{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c612a7b",
   "metadata": {},
   "source": [
    "### Explain the reasons behind Dimension Reduction\n",
    "\n",
    "It reduces the time and storage space required. It helps Remove multi-collinearity which improves the interpretation of the parameters of the machine learning model. It becomes easier to visualize the data when reduced to very low dimensions such as 2D or 3D. It avoids the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c828e",
   "metadata": {},
   "source": [
    "### Define multicollinearity\n",
    "\n",
    "Multicollinearity is the occurrence of high intercorrelations among two or more independent variables in a multiple regression model. Multicollinearity can lead to skewed or misleading results when a researcher or analyst attempts to determine how well each independent variable can be used most effectively to predict or understand the dependent variable in a statistical model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26589bb7",
   "metadata": {},
   "source": [
    "### Define PCA\n",
    "\n",
    "Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.\n",
    "\n",
    "PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf9db62",
   "metadata": {},
   "source": [
    "### What is the difference between PCA and LDA?\n",
    "\n",
    "LDA focuses on finding a feature subspace that maximizes the separability between the groups. While Principal component analysis is an unsupervised Dimensionality reduction technique, it ignores the class label. PCA focuses on capturing the direction of maximum variation in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ac0f6c",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2bc63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "# load dataset into Pandas DataFrame\n",
    "df = pd.read_csv(url, names=['sepal length','sepal width','petal length','petal width','target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105a1236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "features = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "# Separating out the features\n",
    "x = df.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = df.loc[:,['target']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1a5ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
